{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5551dab",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier — Wine Quality Dataset (From Scratch)\n",
    "\n",
    "This notebook demonstrates a **Decision Tree classifier implemented entirely from scratch**\n",
    "using the `rice_ml` package. No `sklearn` models are used.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Load and inspect the Wine Quality dataset\n",
    "- Perform exploratory data analysis (EDA)\n",
    "- Construct a binary classification target\n",
    "- Train a custom Decision Tree classifier\n",
    "- Evaluate model performance\n",
    "- Visualize decision regions using PCA\n",
    "\n",
    "This notebook mirrors a standard supervised learning workflow and emphasizes\n",
    "both **interpretability** and **mathematical intuition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c21253",
   "metadata": {},
   "source": [
    "## Understanding Decision Trees — Intuition and Mathematics\n",
    "\n",
    "A **Decision Tree** is a supervised learning model that predicts outcomes by\n",
    "recursively splitting the feature space into smaller, more homogeneous regions.\n",
    "\n",
    "Each internal node applies a rule of the form:\n",
    "\n",
    "- Is `feature_j ≤ threshold`?\n",
    "\n",
    "Each leaf node stores a class probability distribution and predicts the\n",
    "most likely class.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Decision Trees attempt to create regions where the target variable is\n",
    "as **pure** as possible.\n",
    "\n",
    "For the wine dataset, intuitive rules might include:\n",
    "\n",
    "- Higher alcohol → better quality\n",
    "- Higher volatile acidity → worse quality\n",
    "- Higher sulphates → better quality\n",
    "\n",
    "These rules are **learned automatically** by the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091b4fa",
   "metadata": {},
   "source": [
    "### Gini Impurity\n",
    "\n",
    "To measure how mixed a node is, we use **Gini impurity**:\n",
    "\n",
    "$$\n",
    "G = 1 - \\sum_{c} p(c)^2\n",
    "$$\n",
    "\n",
    "where $p(c)$ is the proportion of class $c$ in the node.\n",
    "\n",
    "\n",
    "- Gini = 0 → pure node\n",
    "- Gini near 0.5 → mixed classes\n",
    "\n",
    "### How Splits Are Chosen\n",
    "\n",
    "At each node, the tree evaluates all possible feature–threshold pairs and\n",
    "chooses the split that **minimizes the weighted Gini impurity** of the children.\n",
    "\n",
    "This greedy process is repeated recursively until stopping criteria are met.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776cc25",
   "metadata": {},
   "source": [
    "## Dataset Overview — Wine Quality (Red)\n",
    "\n",
    "The Wine Quality Red dataset contains **1,599 Portuguese red wines**, each\n",
    "described by **11 continuous physicochemical attributes**:\n",
    "\n",
    "- fixed acidity\n",
    "- volatile acidity\n",
    "- citric acid\n",
    "- residual sugar\n",
    "- chlorides\n",
    "- free sulfur dioxide\n",
    "- total sulfur dioxide\n",
    "- density\n",
    "- pH\n",
    "- sulphates\n",
    "- alcohol\n",
    "\n",
    "The original target variable `quality` is an integer score from 0–10.\n",
    "\n",
    "There are **no missing values** in this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfcc03d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "df = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcc64bc",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before modeling, we explore:\n",
    "\n",
    "- Dataset shape\n",
    "- Distribution of wine quality scores\n",
    "- Feature behavior by class\n",
    "- Correlation structure\n",
    "\n",
    "Although Decision Trees do not require feature scaling,\n",
    "EDA helps interpret which splits the tree may learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500bbdbe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Target Distribution\n",
    "\n",
    "df[\"quality\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.title(\"Wine Quality Distribution\")\n",
    "plt.xlabel(\"Quality Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460a393",
   "metadata": {},
   "source": [
    "## Binary Classification Target\n",
    "\n",
    "To simplify modeling, we convert wine quality into a binary target:\n",
    "\n",
    "- **1 (Good wine)**: quality ≥ 6\n",
    "- **0 (Bad wine)**: quality < 6\n",
    "\n",
    "This creates an interpretable binary classification problem\n",
    "well-suited for Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1119d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature vs. Target Visualization\n",
    "\n",
    "df[\"target\"] = (df[\"quality\"] >= 6).astype(int)\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "for i, col in enumerate(df.drop(columns=[\"quality\", \"target\"]).columns, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.boxplot(\n",
    "        data=df,\n",
    "        x=\"target\",\n",
    "        y=col,\n",
    "        hue=\"target\",\n",
    "        dodge=False,\n",
    "        palette=\"viridis\",\n",
    "        legend=False\n",
    "    )\n",
    "    plt.title(f\"{col} vs Target\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184ac76",
   "metadata": {},
   "source": [
    "### Correlation Structure\n",
    "\n",
    "Decision Trees are insensitive to correlation,\n",
    "but understanding redundancy among features provides insight into the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    df.drop(columns=[\"quality\", \"target\"]).corr(),\n",
    "    cmap=\"coolwarm\",\n",
    "    annot=False\n",
    ")\n",
    "plt.title(\"Correlation Heatmap of Wine Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9acd836",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We construct:\n",
    "\n",
    "- Feature matrix $X \\in \\mathbb{R}^{n \\times d}$\n",
    "- Target vector $y \\in \\{0,1\\}^n$\n",
    "\n",
    "We then split the dataset into training and test sets using\n",
    "our custom `train_test_split` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Split\n",
    "\n",
    "from rice_ml.preprocessing import train_test_split\n",
    "\n",
    "X = df.drop(columns=[\"quality\", \"target\"]).values\n",
    "y = df[\"target\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77f06c",
   "metadata": {},
   "source": [
    "## Training the Decision Tree\n",
    "\n",
    "We train a Decision Tree classifier with a **maximum depth constraint**.\n",
    "\n",
    "Limiting depth helps control overfitting by restricting\n",
    "how many recursive splits the tree can make.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bcab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rice_ml.supervised_learning.decision_tree import DecisionTree\n",
    "\n",
    "tree = DecisionTree(max_depth=4)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "train_acc = tree.score(X_train, y_train)\n",
    "test_acc = tree.score(X_test, y_test)\n",
    "\n",
    "train_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6b048",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- Training accuracy is higher than test accuracy, indicating mild overfitting\n",
    "- The depth constraint provides a reasonable bias–variance tradeoff\n",
    "- The model learns meaningful threshold-based rules from the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6faa064",
   "metadata": {},
   "source": [
    "## PCA for Visualization\n",
    "\n",
    "Because the dataset has 11 features, we cannot visualize\n",
    "decision boundaries directly.\n",
    "\n",
    "We use **Principal Component Analysis (PCA)** to project the data\n",
    "into two dimensions **for visualization only**.\n",
    "\n",
    "PCA finds orthogonal directions of maximum variance:\n",
    "\n",
    "$$\n",
    "X_{\\text{PCA}} = X W\n",
    "$$\n",
    "\n",
    "The Decision Tree itself is still trained in the original feature space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b362252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and 2D Tree\n",
    "\n",
    "from rice_ml.unsupervised_learning.pca import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_2d = pca.fit_transform(X_train)\n",
    "X_test_2d = pca.transform(X_test)\n",
    "\n",
    "tree_2d = DecisionTree(max_depth=4)\n",
    "tree_2d.fit(X_train_2d, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Boundary Visualization\n",
    "\n",
    "x_min, x_max = X_train_2d[:, 0].min() - 1, X_train_2d[:, 0].max() + 1\n",
    "y_min, y_max = X_train_2d[:, 1].min() - 1, X_train_2d[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(x_min, x_max, 300),\n",
    "    np.linspace(y_min, y_max, 300)\n",
    ")\n",
    "\n",
    "Z = tree_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(\"equal\", \"box\")\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.25, cmap=\"viridis\")\n",
    "plt.contour(xx, yy, Z, colors=\"black\", linewidths=0.3)\n",
    "\n",
    "plt.scatter(X_train_2d[:, 0], X_train_2d[:, 1],\n",
    "            c=y_train, cmap=\"viridis\", edgecolor=\"white\", s=40, label=\"Train\")\n",
    "\n",
    "plt.scatter(X_test_2d[:, 0], X_test_2d[:, 1],\n",
    "            c=y_test, cmap=\"viridis\", edgecolor=\"black\", s=80, marker=\"X\", label=\"Test\")\n",
    "\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"Decision Tree Decision Regions (PCA Projection)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc2bea",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we applied a custom Decision Tree classifier\n",
    "implemented from scratch using the `rice_ml` package.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- Decision Trees learn interpretable, threshold-based rules\n",
    "- Limiting tree depth controls overfitting\n",
    "- PCA enables visualization of high-dimensional decision boundaries\n",
    "- Axis-aligned splits are a defining characteristic of trees\n",
    "\n",
    "This notebook serves as a reusable template for analyzing\n",
    "tree-based models within the `rice_ml` framework."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
