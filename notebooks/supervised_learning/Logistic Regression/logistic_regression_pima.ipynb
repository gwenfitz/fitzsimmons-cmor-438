{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression — Pima Indians Diabetes Dataset\n",
    "\n",
    "This notebook demonstrates **binary logistic regression** implemented entirely from scratch using the custom `LogisticRegression` class from the `rice_ml` package.\n",
    "\n",
    "We apply the model to the **Pima Indians Diabetes dataset**, a classic benchmark for binary classification in medical diagnostics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed608e6",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "- Load and inspect a real-world medical dataset  \n",
    "- Perform exploratory data analysis (EDA)  \n",
    "- Standardize features for numerical stability  \n",
    "- Train a logistic regression classifier using gradient descent  \n",
    "- Evaluate performance using accuracy, confusion matrix, and ROC–AUC  \n",
    "- Interpret results from both a statistical and machine learning perspective  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bdb0e5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rice_ml.supervised_learning.logistic_regression import LogisticRegression\n",
    "from rice_ml.processing.preprocessing import standardize, train_test_split\n",
    "from rice_ml.processing.post_processing import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "We will load the Pima Indians Diabetes dataset directly from a public URL.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The **Pima Indians Diabetes dataset** contains medical diagnostic measurements used to predict whether a patient has diabetes.\n",
    "\n",
    "### Features\n",
    "\n",
    "- `pregnancies`: number of pregnancies  \n",
    "- `glucose`: plasma glucose concentration  \n",
    "- `blood_pressure`: diastolic blood pressure (mm Hg)  \n",
    "- `skin_thickness`: triceps skin fold thickness (mm)  \n",
    "- `insulin`: 2-hour serum insulin (mu U/ml)  \n",
    "- `bmi`: body mass index  \n",
    "- `diabetes_pedigree`: diabetes pedigree function  \n",
    "- `age`: age in years  \n",
    "\n",
    "### Target\n",
    "\n",
    "- `label`: binary outcome  \n",
    "  - `1` → diabetes  \n",
    "  - `0` → no diabetes  \n",
    "\n",
    "### Data Notes\n",
    "\n",
    "- All features are numerical  \n",
    "- Several variables contain **zero values that represent missing or implausible measurements**  \n",
    "- No explicit NaN values are present  \n",
    "- Feature scales differ substantially, motivating standardization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "cols = [\n",
    "    \"pregnancies\", \"glucose\", \"blood_pressure\", \"skin_thickness\",\n",
    "    \"insulin\", \"bmi\", \"diabetes_pedigree\", \"age\", \"label\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(url, header=None, names=cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79ff68d",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before modeling, we examine feature distributions and scale differences to understand the structure of the data.\n",
    "\n",
    "### Feature Distributions\n",
    "\n",
    "Histograms reveal skewness, outliers, and non-Gaussian behavior in several features such as insulin and glucose.\n",
    "\n",
    "### Feature Scale Comparison\n",
    "\n",
    "Boxplots highlight large differences in scale across features.\n",
    "Some variables span orders of magnitude larger than others, which can destabilize gradient-based optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc73277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Distributions\n",
    "df.drop(columns=[\"label\"]).hist(figsize=(12, 8), bins=20)\n",
    "plt.suptitle(\"Feature Distributions\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Feature Scale Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "df.drop(columns=[\"label\"]).boxplot(rot=45)\n",
    "plt.title(\"Feature Boxplots (Before Standardization)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75580a",
   "metadata": {},
   "source": [
    "## EDA Interpretation\n",
    "\n",
    "- Features vary widely in scale (e.g., insulin vs diabetes pedigree)  \n",
    "- Several variables are right-skewed and contain outliers  \n",
    "- Logistic regression relies on gradient descent, which is sensitive to feature scale  \n",
    "\n",
    "These observations motivate **feature standardization** prior to model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We separate predictors and target, then standardize the features.\n",
    "\n",
    "Let the feature matrix be:\n",
    "\n",
    "- Feature matrix: $X \\in \\mathbb{R}^{n \\times d}$  \n",
    "- Target vector: $y \\in \\{0,1\\}^n$\n",
    "\n",
    "Feature standardization is defined as:\n",
    "\n",
    "$$\n",
    "X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "This ensures:\n",
    "\n",
    "- Comparable feature scales  \n",
    "- Faster and more stable gradient descent  \n",
    "- Improved numerical conditioning  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"label\"]).values\n",
    "y = df[\"label\"].values.astype(float)\n",
    "\n",
    "X_std = standardize(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c3315",
   "metadata": {},
   "source": [
    "## Logistic Regression — Model Intuition\n",
    "\n",
    "Logistic regression models the probability of the positive class as:\n",
    "\n",
    "$$\n",
    "P(y = 1 \\mid x) = \\sigma(w^\\top x + b)\n",
    "$$\n",
    "\n",
    "where the sigmoid function is:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0a626",
   "metadata": {},
   "source": [
    "### Loss Function (Log-Loss)\n",
    "\n",
    "We minimize the **binary cross-entropy loss**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) =\n",
    "-\\frac{1}{n} \\sum_{i=1}^n\n",
    "\\left[\n",
    "y_i \\log(\\hat{p}_i) + (1 - y_i)\\log(1 - \\hat{p}_i)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This loss is:\n",
    "\n",
    "- Convex  \n",
    "- Differentiable  \n",
    "- Well-suited for probabilistic classification  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We train logistic regression using gradient descent.\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(Xw) = \\frac{1}{1 + e^{-Xw}}\n",
    "$$\n",
    "\n",
    "We minimize the log-loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{n} \\sum_i \\big( y_i\\log(\\hat{y}_i) + (1 - y_i)\\log(1-\\hat{y}_i) \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "    learning_rate=0.1,\n",
    "    max_iter=5000,\n",
    "    C=1.0\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Coefficients:\", model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} =\n",
    "\\frac{1}{n} \\sum_{i=1}^n\n",
    "\\mathbf{1}\\bigl[\\hat{y}_i = y_i\\bigr]\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $y_i$ is the true label  \n",
    "- $\\hat{y}_i$ is the predicted label  \n",
    "- $\\mathbf{1}[\\cdot]$ is the indicator function  \n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix provides class-specific insight into:\n",
    "\n",
    "- False positives (Type I error)  \n",
    "- False negatives (Type II error)  \n",
    "\n",
    "This is particularly important in medical prediction tasks, where false negatives may be costly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve and AUC\n",
    "\n",
    "The **Receiver Operating Characteristic (ROC)** curve evaluates classifier performance across all classification thresholds.\n",
    "\n",
    "- True Positive Rate (TPR):\n",
    "\n",
    "$$\n",
    "\\text{TPR} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- False Positive Rate (FPR):\n",
    "\n",
    "$$\n",
    "\\text{FPR} = \\frac{FP}{FP + TN}\n",
    "$$\n",
    "\n",
    "The **Area Under the Curve (AUC)** summarizes overall discriminative performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, auc = model.roc_curve(X_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "plt.plot([0,1],[0,1], '--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d8a32",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Logistic regression provides a strong and interpretable baseline for binary classification.\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "- End-to-end modeling using **fully custom-built machine learning code**  \n",
    "- The importance of feature scaling for gradient-based optimization  \n",
    "- Evaluation using both threshold-dependent and threshold-independent metrics  \n",
    "\n",
    "Despite its simplicity, logistic regression performs competitively on this dataset and provides probabilistic outputs that are especially valuable in medical decision-making.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
